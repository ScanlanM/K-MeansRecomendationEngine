{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20503812-4d61-47fa-8001-2469e14202c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, Word2Vec, StopWordsRemover\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5006d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = os.listdir('extractandshuffle')\n",
    "filelist[0]\n",
    "shufflefilepathlist=[]\n",
    "for f in filelist:\n",
    "    shufflefilepathlist.append(\"extractandshuffle/\"+f)\n",
    "shufflefilepathlist[0]\n",
    "txtpath='extractandshuffle/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b424343-0ffa-4bc9-86d0-68457c065525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSearch:\n",
    "    def __init__(self, filepathlist,txtpath,filelist):\n",
    "        self.name = filepathlist\n",
    "        self.txtpath = txtpath\n",
    "        self.filelist = filelist\n",
    "        self.saveModelName =\"KmeansModelPipeline.model\"\n",
    "\n",
    "    \n",
    "    def startSpark(self): #starts spark session and defines base schema for dfs\n",
    "        os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "        os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "        self.sc = SparkContext(appName=\"k\") #not yet implmented handling of existing spark sessions\n",
    "        self.spark = SparkSession(self.sc) #kernel must be reset to reinitiate spark to create new class instance\n",
    "        self.spark.sparkContext.setLogLevel(\"FATAL\")\n",
    "        self.schema = schema = StructType([\n",
    "            StructField(\"filename\", StringType(), True),\n",
    "            StructField(\"filepath\", StringType(), True),\n",
    "            StructField(\"contents\", StringType(), True)])\n",
    "        \n",
    "    def pullfile(self,file): #Function used in make_spark_df to pull text files\n",
    "        with open(file,encoding='utf-8') as f:\n",
    "            chapStr=f.read()\n",
    "        return chapStr\n",
    "\n",
    "    def make_spark_df(self): #Function used to aggregate content of text files into a dataframe\n",
    "        records = []\n",
    "        for file in self.filelist:\n",
    "            filepath=self.txtpath+file\n",
    "            contents = self.pullfile(filepath)\n",
    "            record={'filename':file,'filepath':filepath,'contents':contents}\n",
    "            records.append(record)\n",
    "        aDF = self.spark.createDataFrame((Row(**x) for x in records),self.schema)\n",
    "        return aDF\n",
    "    \n",
    "    def createFrames(self): #Function used to create dataframes\n",
    "        self.df = self.make_spark_df().sample(fraction=.1) #creates main database of documents that can be reccomended\n",
    "        self.searchDF = self.make_spark_df().sample(fraction=.001)\n",
    "    \n",
    "    def fit_pipeline(self): #creates stages of piplines fits pipline to document database\n",
    "        self.tokenizer= RegexTokenizer(inputCol=\"contents\", outputCol=\"tokens\",minTokenLength=4)\n",
    "        self.stop = StopWordsRemover(inputCol=\"tokens\",outputCol=\"filteredtokens\")\n",
    "        self.w2v = Word2Vec(inputCol=\"filteredtokens\", outputCol=\"features\")\n",
    "        self.kmeans = KMeans(maxIter=5,featuresCol='features',predictionCol='prediction').setK(1500)\n",
    "        self.stages=[self.tokenizer,self.stop,self.w2v,self.kmeans]\n",
    "        self.content_processing_pipe = Pipeline(stages=self.stages).fit(self.df)\n",
    "        \n",
    "    def transform_ddb(self):# transforms document database with predictions to compare with search input\n",
    "        self.df = self.content_processing_pipe.transform(self.df).select('filename','filepath','prediction')\n",
    "        self.df = self.df.drop(\"tokens\",\"filteredtokens\",\"features\",\"content\") #removes unused columns to free up space\n",
    "        # currently content column is also removed as the functionality to pull recommeneded content has not been implemented\n",
    "        # content could be pulled from the origin text file using the filepath column rather than from df in volitle memory\n",
    "    \n",
    "    def writeModel(self): #attempts to save model\n",
    "        try: #Reusing model not yet implemented\n",
    "            self.content_processing_pipe.save(self.saveModelName)\n",
    "        except:\n",
    "            print(\"save failed\")\n",
    "    \n",
    "    def make_predictions(self, search=None): #takes in a document and predicts which cluster it will relate to\n",
    "        if search != None:\n",
    "            self.predictions = self.content_processing_pipe.transform(search).select('filename','filepath','prediction')\n",
    "        else:\n",
    "            self.predictions = self.content_processing_pipe.transform(self.searchDF).select('filename','filepath','prediction')\n",
    "        \n",
    "    def isolatesearch(self): #ensures that only one document is searched\n",
    "        self.userSearch=self.predictions.tail(1) #batch searches not implemented\n",
    "    \n",
    "    def search_results(self): #extracts predicted cluster label from users search\n",
    "        clusterLabel=str(self.userSearch[0][2])\n",
    "        print(clusterLabel)\n",
    "        results = self.df.filter(self.df['prediction'] == clusterLabel)\n",
    "        return results\n",
    "                      \n",
    "    def reccomend(self): # returns a reccomendation to the user\n",
    "        results = self.search_results().collect()\n",
    "        print(\"If you liked: \" +self.userSearch[0][0]) #returns file name of users document used for search\n",
    "        print(\"Then you may like:\")\n",
    "        for result in results:\n",
    "            print(result[0]) #returns filenames of cluster members\n",
    "    \n",
    "    def rt(self):\n",
    "        now = datetime.now()\n",
    "        print(\"time = \", now)\n",
    "    \n",
    "    def search(self): #main function of class that builds pipline model and simulates user input by selecting\n",
    "        print(\"Starting Spark\")\n",
    "        self.rt()\n",
    "        self.startSpark()\n",
    "        print(\"creating spark DFs\")\n",
    "        self.createFrames()\n",
    "        print(\"Building model\")\n",
    "        self.fit_pipeline()\n",
    "        print(\"Creating base dataset\")\n",
    "        self.transform_ddb()\n",
    "        print(\"saving model\")\n",
    "        self.writeModel()\n",
    "        print(\"searching for similar documents\")\n",
    "        self.make_predictions()\n",
    "        self.isolatesearch()\n",
    "        self.reccomend()\n",
    "        self.rt()\n",
    "        \n",
    "    def newSearch(self,document=None):\n",
    "        print(\"searching for similar documents\")\n",
    "        if document != None:\n",
    "            self.sampleDF = document\n",
    "        else:\n",
    "            count = self.searchDF.count()\n",
    "            x = 10/count\n",
    "            sampleDF = self.searchDF.sample(fraction=x)\n",
    "        self.make_predictions(sampleDF)\n",
    "        self.isolatesearch()\n",
    "        self.reccomend()\n",
    "        self.rt()\n",
    "        \n",
    "    def load_existing_search(self): #not implemented function to load in existing Kmeans pipline save to preform searches\n",
    "        pass #would use a sub class to load in model and inherit document database for user instance searching\n",
    "            #would also add in additional functionality for handling user provided documents\n",
    "            #implementation of spark streaming would allow for distributed computation of user searches to scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750e813d-fac1-407a-8f85-52f2e3aa58c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark\n",
      "time =  2022-03-04 13:18:10.236066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Xmx30G\n",
      "Picked up _JAVA_OPTIONS: -Xmx30G\n",
      "22/03/04 13:18:11 WARN Utils: Your hostname, matthew-MS-7A34 resolves to a loopback address: 127.0.1.1; using 192.168.0.232 instead (on interface wlp40s0)\n",
      "22/03/04 13:18:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/04 13:18:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating spark DFs\n",
      "Building model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating base dataset\n",
      "saving model\n",
      "save failed\n",
      "searching for similar documents\n",
      "931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=====================>                                  (6 + 10) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you liked: _book_138_chap_00935.txt\n",
      "Then you may like:\n",
      "_book_138_chap_00295.txt\n",
      "_book_138_chap_00695.txt\n",
      "_book_138_chap_00468.txt\n",
      "_book_138_chap_00595.txt\n",
      "_book_138_chap_01104.txt\n",
      "_book_138_chap_01063.txt\n",
      "_book_138_chap_01216.txt\n",
      "_book_138_chap_00843.txt\n",
      "_book_138_chap_00939.txt\n",
      "_book_138_chap_01107.txt\n",
      "_book_138_chap_00764.txt\n",
      "_book_138_chap_00680.txt\n",
      "_book_138_chap_00354.txt\n",
      "_book_138_chap_00912.txt\n",
      "_book_138_chap_00911.txt\n",
      "_book_138_chap_00080.txt\n",
      "_book_138_chap_01173.txt\n",
      "_book_138_chap_01060.txt\n",
      "_book_138_chap_01168.txt\n",
      "time =  2022-03-04 13:24:34.544157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "s = KSearch(shufflefilepathlist,txtpath,filelist)\n",
    "s.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72a5b48e-6fb5-46bb-852a-74e3cbb9e825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for similar documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you liked: _book_54_chap_00515.txt\n",
      "Then you may like:\n",
      "_book_54_chap_00187.txt\n",
      "_book_54_chap_00723.txt\n",
      "_book_54_chap_00013.txt\n",
      "_book_54_chap_00105.txt\n",
      "_book_54_chap_00088.txt\n",
      "_book_54_chap_00150.txt\n",
      "_book_54_chap_00673.txt\n",
      "_book_54_chap_00761.txt\n",
      "_book_54_chap_00205.txt\n",
      "_book_54_chap_00108.txt\n",
      "_book_54_chap_00820.txt\n",
      "_book_54_chap_00771.txt\n",
      "_book_54_chap_00243.txt\n",
      "_book_54_chap_00347.txt\n",
      "_book_54_chap_00482.txt\n",
      "_book_54_chap_00833.txt\n",
      "time =  2022-03-04 13:24:43.428874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "s.newSearch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
